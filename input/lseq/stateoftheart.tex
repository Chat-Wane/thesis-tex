
\section{État de l'art}

\label{lseq:sec:stateoftheart}

\subsection{Réplication optimiste}

La réplication optimiste~\cite{demers1987epidemic, saito2005optimistic} est un
paradigme de réplication qui consiste à copier la donnée partagée chez chaque
utilisateur. De cette façon, ces derniers peuvent directement modifier les
copies, et ce, même en cas de déconnexions.  Ainsi, les données sont toujours
disponibles et réactives aux changements effectués. Dans un second temps, les
modifications sont disséminées aux autres possesseurs de cette donnée partagée
où elles sont appliquées à la copie locale.

D'après le théorème CAP~\cite{gilbert2002brewer} (\emph{Consistency,
  Availability, Partition tolerence}), il est impossible de passer à l'échelle
tout en garantissant à la fois
\begin{itemize}
\item la cohérence : un contrat entre le développeur et la structure qui
  spécifie comment cette dernière se comporte suivant les opérations effectuées
  et leur ordonancement. Les contraintes imposées à la structure peuvent être
  plus ou moins importantes selon les besoins.
\item la disponibilité : le ratio entre le temps effectif durant lequel
  l'utilisateur accède à un service et le temps durant lequel il souhaite y
  accèder. Dans le meilleur cas, le service est toujours disponible. \TODO{La
    plupart des services \emph{Cloud} proposent de 99 à 100\% (exclus) de
    disponibilité.}
\item la tolérance aux pannes : les défaillances n'entrainent pas de defauts
  dans les propriétés susmentionnées.
\end{itemize}

La réplication optimiste choisit de sacrifier sur le critère de cohérence au
profit de la disponibilité et de la tolérance aux pannes: lorsque les mêmes
changements ont été reçus et appliqués par tous les participants, les copies
convergent vers un état équivalent. Il s'agit du critère de cohérence
correspondant à la cohérence à terme forte~\cite{shapiro2011conflict} (ou
cohérence inéluctable forte). Bien qu'il soit possible de garantir d'avantages
de propriétés, notamment sur l'ordonnancement des modifications, nous nous
intéresserons principalement à ce critère de cohérence.

\begin{figure}
  \centering
  \input{./input/lseq/optimisticexample.tex}
  \caption{\label{lseq:fig:optimisticexample}Exemple d'exécution d'un protocole
    de réplication optimiste. Il existe trois copies d'une séquence initialement
    vide. La première copie insère 'QWE' et en dissémine l'information. La
    troisième copie reçoit l'opération et l'applique localement. Cette copie
    insère 'RTY' à la suite de 'QWE' afin d'obtenir 'QWERTY' et envoie
    l'information aux deux autres copies. Quel que soit l'ordre de réception, le
    protocole garanti que les copies convergent vers un état identique, ici, la
    séquence 'QWERTY'.}
\end{figure}

La figure~\ref{lseq:fig:optimisticexample} présente un cas de séquence
répliquée. Quel que soit l'ordre de reception des opérations et leur nature, les
copies convergent toute vers un état identique : la séquence QWERTY.

Les approches de réplication optimiste pour séquences se divisent en deux
catégories. Les premiers utilisent les transformés
opérationnels~\cite{sun1998operational, sun2009contextbased} qui, lors de la
réception d'une modification, en change les paramètres égards des opérations
concurrentes. Les seconds utilisent un type de données
commutatif~\cite{shapiro2011comprehensive, shapiro2011conflict} où l'envoie
n'est pas l'opération mais son résultat qui, après réception, est intégré à la
structure.

\subsection{Transformés opérationnels}

Les approches à transformés opérationnels~\cite{sun1998operational,
  sun2009contextbased} (OT) sont les plus anciennes et s'appliquent à un large
champs d'applications tels que l'édition de texte, l'édition d'images etc. Dans
le cadre de l'édition de texte, en plus des usuelles opérations d'insertion et
de suppression, OT founit des opérations ciblant les chaînes de caractères
telles que le déplacement, le couper -- coller, etc. Toutefois, l'analyse de
correction nécessite d'examiner chaque paire d'opérations ainsi que leurs
paramètres. En conséquence, lors de l'écriture du papier
\cite{imine2003proving}, peu d'approches étaient réellement correctes. De plus,
cette classe d'approches peut être divisé en deux sous-classes: les approches
centralisées et les approches décentralisées. Les premières réarrangent les
opérations sur un serveur central~\cite{nichols1995high} afin de faciliter la
convergence. Toutefois, la topologie elle-même implique un point individuel de
défaillance, des problèmes de confidentialité, des problèmes d'intelligence
économique, des problèmes de censure, et enfin, de passage à l'échelle. Les
approches décentralisées~\cite{sun2009contextbased}, quant à elle, nécessitent
un vecteur de version afin d'identifier les contextes de génération des
opérations reçues. Elles transforment les arguments de l'opération reçue par
rapport aux opérations concurrentes dans le but d'exécuter de manière cohérente
l'opération sans avoir à défaire et réexecuter ces opérations. De ce fait, bien
que l'exécution locale d'une opération soit très efficace, l'exécution des
opérations reçues est très coûteuse en cas de concurrence. Ainsi, confiné aux
environnements maîtrisés, OT reste efficace~\cite{mehdi2014merging}.

\begin{figure}
  \centering
  \input{input/lseq/otexample.tex}
  \caption{\label{lseq:fig:otexample}Exemple de transformé opérationnel
    garantissant la convergence lors d'opérations concurrentes. L'opération de
    suppression des 3 premiers caractères sur la copie 3 (RTY) est transformée
    afin de supprimer les 3 caractères à l'index 3 sur les autres copies.}
\end{figure}

La figure~\ref{lseq:fig:otexample} illustre le principe de fonctionnement des
approches basées sur les transformés opérationnels. Dans ce scenario, les copies
sont toutes initialisées avec la séquence RTY. Ensuite, tandis que la copie 1
insère les 3 caractères QWE en tête de la séquence pour obtenir QWERTY, la copie
3 supprime ses trois caractères pour obtenir la séquence vide. Avant la
dissémination de ces opérations, les copies ne sont pas identiques. Les copies
1, 2, et 3 ont respectivement les séquences [QWERTY], [RTY], []. Lorsque la
copie 1 réçoit l'opération de suppression, elle détecte que cette dernière est
en concurrence avec des opérations déjà intégrées, en l'occurence,
$insert(QWE,\,0)$. Son objectif est alors de déterminer l'impact que cette
opération concurrente a eu sur l'opération réçue afin d'en adapter les
arguments. Ici, l'insertion a décalé la séquence RTY de 3 positions vers la
droite. Par conséquent, La suppression, de part sa position, est elle aussi
décalée de 3 positions vers la droite. La résultat de la transformation est
$delete(3,\,3)$.  La copie 3, lorsqu'elle réçoit l'opération d'insertion,
detecte elle aussi que cette dernière est concurrente. Toutefois, la
transformation est sans effet sur ses arguments. La copie 2, selon l'ordre de
réception, se comporte comme la copie 1 ou la copie 3. À terme, les trois copies
convergent vers une séquence identique QWE.

\subsection{Structures de données sans résolution de conflits}

Plus récemment, les approches à base de \emph{structures de données répliqués
  sans résolution de conflits}~\cite{shapiro2011comprehensive,
  shapiro2011conflict} (CRDTs) commencèrent à émerger. Ces types de structure
abstraits possèdent la particularité de fournir des opérations dont les
résultats sont commutatifs entre eux.  En d'autres termes, le résultat de
l'exécution locale de chaque opération est envoyé aux possesseur de réplique et
peut directement être intégré à cette dernière.  L'ordre d'intégration des
opérations n'importe pas. Cela permet de grandement alléger, voir supprimer, le
coût d'ordonnancement des opérations. Ce genre de structure existe pour les
compteurs, les ensembles, les graphes orientés acycliques (DAG) etc.

Une importante différence entre les approches basées sur OT et les approches
basées sur les CRDTs réside dans la répartition des coûts algorithmiques entre
exécution locale et intégration distante. En effet, OT propose des opérations
sans coût additionnel localement, mais dont le coût est élevé à l'intégration.
En revanche, les approches CRDTs répartissent les coûts entre la partie locale
et l'intégration.  L'impact étant d'autant plus important qu'une opération local
génère autant d'intégrations que de répliques.

\TODO{Tandis que les CRDTs améliorent de manière significative la complexité
temporelle des opérations comparé aux approches OT décentralisées, ils
dissimulent leur complexité dans l'occupation mémoire. En effet, afin d'assurer
la convergence des répliques, une opération d'insertion alloue un identifiant
unique.}

\subsubsection{Le compteur}

L'exemple du CRDT pour compteur permet d'introduire l'idée sur laquelle ils se
fondent. \TODO{Par la suite, on note
  $op_a \times op_b \times \ldots \times op_n$ une séquence d'opérations.}


Un compteur est simplement un entier proposant aux utilisateurs de l'incrémenter
de 1. Si 3 utilisateurs incrémentent chacun leur compteur, les répliques du
compteur convergeront inéluctablement vers la valeur 3. Dans ce cas, il est
clair que l'ordre d'intégration des opérations importe peu :
\begin{equation}
  Inc_a \times Inc_b = Inc_b \times Inc_a \, \, \, (commutative)
\end{equation}

Ainsi, l'origine de l'incrémentation ($a$ ou $b$) n'influence pas le résultat.
Toutefois, il est nécessaire de s'assurer qu'aucune opération n'est appliquée
plusieurs fois. Ainsi, chaque opération est identifiée de manière unique de telle
sorte que :
\begin{equation}
  Inc_a \times Inc_a = Inc_a \, \, \, (idempotence)
\end{equation}

Les identifiants accompagnant chaque opération constituent le surcoût des
approches CRDTs. D'une manière ou d'une autre, il est nécéssaire de les
enregistrer afin de garantir que la structure progresse continuellement. Par
exemple, l'implémentation de ce compteur peut se faire sous la forme d'un
ensemble répertoriant tous les identifiants. Sa valeur est le cardinal de cet
ensemble.

Considérons maintenant que le compteur propose une opération supplémentaire, à
savoir la décrémentation de 1. Une implémentation naïve consisterait à supprimer
un élément de l'ensemble aléatoirement. Toutefois, si deux utilisateurs
suppriment en concurrence (autrement dit en même temps) le même élément, alors
le compteur ne serat globalement décrémenté que de 1. Une autre implémentation
plus coûteuse consiste à associer un identifiant unique par décrémentation et
les placer dans un ensemble différent. La valeur du compteur est alors égale au
cardinal des incrémentations moins le cardinal des décrémentations. Cela
correspond aux approches à pierre tombales décritent en
section~\ref{lseq:subsubsec:tombstones}. La structure grandit linéairement par
rapport aux opérations effectuées sur celle-ci.  

Une autre implémentation de ce compteur consiste à sauvegarder un vecteur d'état
recensant le nombre d'incrémentations et de décrémentations effectuées par
chaque utilisateur. Ainsi, la structure grandit linéairement par rapport au
nombre d'utilisateurs ayant jamais effectué une opération sur le compteur
réparti (\TODO{max(received value)}). Cette optimisation est possible car aucune
des opérations ne dépend d'une autre pour être menée à bien. La
section~\ref{lseq:subsubsec:variable} présente un autre type d'optimisation lié
à la séquence.

Le cas du CRDTs pour compteur met en lumière qu'il existe plusieurs
implémentations proposant des compromis différent.  Toutefois, quelle que soit
la structure, elles coûtent beaucoup plus cher que leurs homologues
non-répartis. Il est nécéssaire d'analyser avec prudence les besoins de
l'application afin de choisir au mieux son compromis entre temps, espace, et
communication. La suite de ce chapitre s'attache à décrire les approches CRDTs
dédiées aux séquences.

\subsubsection{Pierres tombales}
\label{lseq:subsubsec:tombstones}

Les approches utilisant des pierres tombales~\cite{ahmed2011evaluating,
  conway2014language, grishchenko2010deep, oster2006data,
  preguica2009commutative, roh2011replicated, weiss2007wooki, wu2010partial,
  yu2012stringwise} se caractérisent par la manière dont les suppressions sont
traitées. En effet, lors de l'opération $delete$, ces approches marquent
simplement les éléments supprimés afin de les cacher à l'utilisateur. Bien que
supprimées, ces pierres tombales existent toujours dans la structure
représentant la séquence et continuent d'impacter sur les performances.

Le premier représentant historique appartenant à cette famille de CRDT se nomme
WOOT~\cite{oster2006data}. Lors de l'insertion d'un élément dans la séquence,
l'identifiant généré référence simplement les identifiants voisins à
l'insertion. Par exemple, considérons la chaîne QWETY dont les identifiants
respectifs à chaque caractère sont $i_Q$,$i_W$,\ldots,$i_Y$. Lors de l'insertion
du caractère R entre les caractères E et T, l'identifiant généré est composé de
$i_E$ et $i_T$ respectivement référencés comme étant la borne inférieur et
supérieur du nouvel élément. Lors de l'intégration, un diagramme de Hasse permet
de retrouver l'ordre des éléments de la séquence, même en présence d'opérations
concurrentes. 

\begin{figure}
  \centering
  \input{input/lseq/wootexample.tex}
  \caption{\label{lseq:fig:wootexample}Le diagramme de Hasse du modèle WOOT
    représentant la séquence QWERTY. Tout d'abord, la chaîne de caractères
    AZERTY fut écrite. Les caractères AZ sont supprimés et remplacé par QW, d'où
    l'embranchement. Bien que supprimé, le caractère Z est indispensable au bon
    ordonnancement de la séquence.}
\end{figure}

La figure~\ref{lseq:fig:wootexample} illustre la nécessité de conserver les
pierres tombales. Elle montre le diagramme de Hasse généré lors du scenario
suivant : Tout d'abord, un utilisateur écrit AZERTY. Ensuite, les deux premiers
caractères sont supprimés afin d'être remplacés par les caractères QW. La
séquence finale est QWERTY. Toutefois, les identifiants ne sont pas modifiables,
et l'identifiant du caractère E référence l'identifiant de Z, lui-même
référençant l'identifiant de A. Par conséquent, supprimer complètement les
identifiants de A et/ou de Z revient à rendre l'identifiant de E non
positionnable, et tout ceux qui en dépendent par transitivité.

La complexité de ces approches dépend de l'ensemble des opérations d'insertions
ayant jamais été effectué sur le document. Cela devient problématique dans
certains documents particulièrement assujettis à vandalisme (e.g. Wikipedia). La
conséquence étant qu'un document n'ayant en apparence que peu de contenu
consomme beaucoup de ressources du fait des nombreuses pierres tombales cachées.

Des méchanismes liés au ramasse-miettes décentralisés (REF) peuvent être
déployés afin de pallier ces problèmes de pierres tombales. Toutefois, ceux-ci
sont extrêmement coûteux, la difficulté étant qu'un élément ne peut être
entièrement supprimé que si toutes les répliques \TODO{l'ont bel et bien
  supprimé}. De cette façon, plus aucun nouvel identifiant ne peut désormais le
référencer et l'ordonnencement n'en dépend plus. \TODO{Obtenir cette
  connaissance requière de savoir l'état de chaque réplique
  distante}. \TODO{Contrainte de topologie.}

\subsubsection{Identifiants de taille variable}
\label{lseq:subsubsec:variable}

Les approches utilisant des identifiants de taille
variable~\cite{andre2013supporting, preguica2009commutative, weiss2009logoot}
sont caractérisées par la forme de leurs identifiants dont la taille, comme leur
nom l'indique, peut varier à la génération. Ainsi, la taille d'un identifiant
peut être différente de la taille d'un autre identifiant. Néanmoins, cela
n'affecte pas leur caractère immuable après génération.

Contrairement aux approches basées sur les pierres tombales, les éléments
supprimés disparaissent complètement de la structure. En contrepartie, les
identifiants encodent un ordre total permettant de les placer dans le document
indépendamment des autres éléments. La complexité spatiale de ces identifiants
est cruciale et dépend du nombre d'opérations d'insertion effectuées sur le
document.

\begin{algorithm}
  \input{input/lseq/crdtabstractalgo.tex}
  \caption{\label{algo:lseq:crdtabstract} Patron des algorithmes de séquences avec
    identifiants de taille variable.}
\end{algorithm}

L'algorithme~\ref{algo:lseq:crdtabstract} présente les grandes lignes des CRDTs pour
séquences dont les identifiants ont une taille qui diffère lors de la
génération. L'algorithme est divisé en deux parties qui correspondent à
l'exécution locale et l'exécution distante de la réplication
optimiste. Celles-ci sont elles-mêmes divisées en deux types d'évènements
correspondant aux opérations d'insertions et de suppression d'un
élément. L'algorithme met en lumière trois points :
\begin{itemize}
\item La signature des opérations est différentes de celle des opérations sur
  les séquences non-partagées. En effet, la position d'insertion d'un élément est
  désignée par deux bornes que sont les éléments adjacents à l'insertion, au lieu
  d'un indice dans la séquence.
\item Un identifiant est généré localement lors de l'insertion d'un élément. Par
  conséquent, la complexité est répartie entre la génération locale de
  l'identifiant, et son positionnement à distance.
\item La fonction $allocPath$ désigne la fonction servant à générer un chemin
  dans l'arbre. La fonction $allocDes$ garantie l'unicité des identifiants ainsi
  que la possibilité d'insérer entre deux identifiants même si ces derniers ont
  un chemin identique.  Ce cas peut se présenter lors d'insertions concurrentes.
\end{itemize}


\subsubsection{Stratégie d'allocation d'identifiants}

Dans la littérature, deux genres de stratégies d'allocations d'identifiants
existent. Tout d'abord, considérons l'insertion de l'élément $b$ entre les
éléments $a$ et $c$ dont les identifiants respectifs sont $id_a$ et $id_c$ avec
$id_a<id_c$. Dans tous les cas, une fonction d'allocation cherche à allouer le
plus petit identifiant possible. Ainsi, l'identifiant généré à une taille
maximum de $max(|id_a|,\, |id_c|)+1$. La première stratégie consiste à allouer
une position aléatoire entre les deux identifiants afin d'éviter au maximum les
conflits dûs à la concurrence. Toutefois, cette stratégie consume en moyenne la
moitié de l'espace entre deux identifiants. La seconde stratégie provient
d'observations faites sur un corpus de textes favorable à l'édition de gauche à
droite. Dans ce genre de cas, la stratégie consiste à rapprocher les
identifiants générés du précédent identifiant (e.g. $id_b$ proche de
$id_a$). Toutefois, lorsque le comportement d'édition ne suit pas celui attendu,
la taille des identifiants croît très rapidement.

\begin{figure*}
  \centering
  \subfloat[Allocation quasi-optimale]
  [\label{fig:lseq:allocpathexampleA}Cas d'une allocation quasi-optimale]
  {\input{./input/lseq/allocpathexampleA.tex}}
  \hspace{40pt}
  \subfloat[Pire cas d'allocation]
  [Pire cas d'allocation]
  {\input{./input/lseq/allocpathexampleB.tex}}
  \caption{\label{fig:lseq:allocpathexample} Deux arbres remplis d'identifiants
    résultant de deux séquences d'édition différentes et dont la séquence finale
    est identique : $QWERTY$. L'allocation des identifiants se fait selon le
    même algorithme qui alloue la branche la plus à gauche de l'arbre. L'arbre
    quasiment optimal ne possède que des branches de profondeur 1 tandis que
    l'arbre pire cas atteint une profondeur de 6.}
\end{figure*}

La figure~\ref{fig:lseq:allocpathexample} illustre les difficultés rencontrées lors
de l'allocation d'identifiants. La figure représente la structure d'arbre
permettant de représenter deux séquences utilisant la fonction d'allocation
suivante : la branche la plus à gauche et la plus petite profondeur de l'arbre
possible. Dans les deux cas, la séquence finale est QWERTY. Toutefois, les
lettres ne sont pas insérées dans un ordre identique. Dans le premier cas, Q est
inséré à l'index 0, suivit de W à l'index 1, suivit de E à l'index 2 etc. Cette
séquence d'opérations d'insertions $[(Q,\,0)$, $(W,\,1)$, $(E,\,2)$\ldots$]$ est
nommée \emph{séquence d'édition}. Dans le second cas, la lettre Y est inséré à
l'index 0, suivit du T à l'index 0 qui va décaler le Y en index 1, etc. La
séquence d'édition qui correspond à ce cas est : $[(Y,\,0)$, $(T,\,0)$,
$(R,\,0)$\ldots$]$.
\begin{itemize}
\item Cas n°1 : la fonction $allocPath$ alloue la branche la plus à gauche
  possible. Par conséquent, la séquence d'édition $[(Q,\,0)$, $(W,\,1)$,
  $(E,\,2)$\ldots$]$ conduit aux chemins suivant $\langle [1],\,Q\rangle$,
  $\langle [2],\, W \rangle$, $\langle [3],\, E\rangle$, etc. Dans ce cas, la
  profondeur de l'arbre n'augmente jamais. À cet égard, la fonction $allocPath$
  est très efficace.
\item Cas n°2 : la séquence d'édition $[(Y,\,0)$, $(T,\,0)$, $(R,\,0)$\ldots$]$
  implique une augmentation de la profondeur de l'arbre à chaque nouvelle
  insertion d'élément. En effet, le chemin est choisit est toujours celui qui
  est le petit possible. Les éléments suivant ne bénéficie pas de suffisamment de
  place à la profondeur courante de l'arbre, d'où la nécessité d'augmenter sa
  profondeur. Les chemins en résultant sont : $\langle [1],\, Y\rangle$,
  $\langle[0.1],\,T\rangle$, $\langle[0.0.1],\, R\rangle$, etc. La taille des
  chemins alloués augmente très rapidement.
\end{itemize}

Cet exemple montre à quel point l'ordre d'insertion des éléments affecte la
longueur des chemins alloués. Malheureusement, ni l'ordre d'insertion des
éléments, ni la taille finale de la séquence ne peuvent être prédits avec
exactitude. C'est pourquoi les travaux précédents font souvent l'hypothèse d'un
comportement d'édition de gauche-à-droite basés sur observations. Toutefois, il
existe des documents écrit par l'homme dont le comportement ne correspond pas à
celui-ci.

\begin{figure*}
  \centering
  \subfloat[Comportement d'édition attendu]
  [\label{fig:lseq:compliant}Le comportement d'édition correspond aux attentes
  de la stratégie d'allocation]
  {\includegraphics[width=0.48\textwidth]{./img/lseq/compliant.eps}}
  \hspace{10pt}
  \subfloat[Comportement d'édition inattendu]
  [\label{fig:lseq:motivating}Le comportement d'édition va à l'encontre des attentes
  de la stratégie d'allocation]
  {\includegraphics[width=0.48\textwidth]{./img/lseq/motivating.eps}}
  \caption{\label{fig:lseq:allocation}Spectre de documents Wikipedia sous différent
    comportements d'édition antagonistes. La figure du haut représente la
    révision à laquelle la ligne a été insérée, i.e., sa date de naissance.  La
    figure du bas représente la taille de l'identifiant associé à chaque ligne.}
\end{figure*}

Les figures~\ref{fig:lseq:compliant} et~\ref{fig:lseq:motivating} montrent deux
comportements d'éditions présent sur des pages extraites de Wikipedia. La partie
supérieure de ces figures donne une vue globale du comportement d'édition sur la
page. Elle indique la numéro de la révision à laquelle une ligne a été
insérée. Ainsi, plus une barre est haute, plus la ligne a été insérée
récemment. Le spectre de la figure~\ref{fig:lseq:compliant} montre que les
nouveaux éléments de la séquence sont principalement ajoutés en fin. À l'opposé,
le spectre de la figure~\ref{fig:lseq:motivating} montre que les nouveaux
éléments de la séquence sont principalement ajoutés en tête. La partie
inférieure de ces figures montre la taille de la représentation binaire de
l'identifiant associé à chaque élément du document. \TODO{Présenter la stratégie
  d'allocation}. Ainsi, nous observons que les identifiants dans le document
comportant 12k lignes mais principalement édité en fin a des identifiants
n'excédant pas 256 bits. En revanche, le document possédant seulement 170 lignes
édité en tête a des identifiants atteignant déjà les 320 bits.

\TODO{Définition du problème}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../paper"
%%% End:
