\section{État de l'art}
\label{sec:relatedwork}

%% requière contexte de distribution

%% #1 réplication optimiste
La réplication optimiste (ou réplication paresseuse) est une strategie de
réplication où chacun des utilisateurs possède une copie locale des données et
peut directement effectuer des modifications sur celle-ci. Le résultat de
chaque modification est ensuite disséminé à tous les utilisateurs de la donnée
partagée. L'accès aux données et la réactivité sont alors guarantis. Par
exemple, le gestionnaire de versions décentralisé nommé Git (REF), ou la base
de données répliquées Riak (REF) sont des représentants de cette classe de
réplication.

%% #2 brewer's theorem
Le théorème CAP (\emph{Consistency, Availability, Partition tolerence}) (REF)
démontre qu'il est impossible pour une application répartie de garantir à la
fois
\begin{inparaenum}[(i)]
\item un critère de cohérence fort (DEFINIR FORT) (REF),
\item une disponibilité sans failles, et
\item une résilience aux partitions réseaux.
\end{inparaenum}
Dans le cadre d'un éditeur collaboratif, le critère de disponibilité est
important, et les partitions réseaux inévitables. Sachant cela, le critère
concernant la cohérence forte est sacrifié.

%% #3 cohérence
Le critère de cohérence auquel on aspire se nomme \emph{cohérence à terme}. La
cohérence à terme spécifie simplement que toutes les répliques d'une donnée
partagée convergent vers un état identique après réceptions de tous les
messages emis. Il s'agit de l'un des critères de cohérence les plus faible qui
existe puisqu'il ne contraint même pas la valeur de l'état de convergence (REF
Mathieu). (EXEMPLE). Cependant, en pratique, celui-ci s'avère utile car il
délègue ces préocupations au developpeur d'applications. Ce dernier peut donc
l'ajuster aux besoins specifiques rencontrés, et payer un coût qui est fonction
de ces besoins.


\subsection{Transformés operationnels}
Dans le cadre d'un éditeur collaborative répartis, les plus anciennes approches
appartenant à la réplication optimiste se nomment \emph{transformés
  operationnels} (OT) (REF). Le principe de celles-ci consiste à envoyer les
opérations locales aux autres editeurs qui sont ensuite chargés de les
executer, mais en les transformant vis-à-vis du document courant. (EXEMPLE).
(TP1 et TP2).

Ces approches définissent un ensemble d'opérations applicables sur le
document. Outre l'insertion et la suppression de chaines de caractères,
certaines approches OT (REF) proposent des opérations plus complexes, comme par
exemple la copie ou le déplacement de chaines de caractères. Toutefois, due au
règle de transformations, cette large gamme d'opérations implique un nombre de
couple d'opérations à spécifier d'autant plus grand. Le developpeur doit se
montrer prudent afin d'assurer la correction de son éditeur. (GOOGLE WAVE).

Les approches OT proposent un compromis sur la complexité temporelle des
opérations. En général, l'opération locale possède la même complexité
temporelle que son homologue non-répartie. En revanche, les opérations
distantes sont plus couteuses et fonction du nombre d'opérations à transformer.
Hélas, ce dernier dépend du contexte d'utilisation et peut s'avérer difficile à
prédire. Il est fonction de la latence réseau, du nombre d'utilisateurs, du
type de document, du logiciel utilisé, etc.

\subsection{Type commutatifs}
Plus récement sont apparus les approches basées sur les \emph{types de données
  répliqués sans résolution de conflits} (CRDTs) (REF). Il existe des CRDTs
pour les ensembles, les compteurs, les graphes, les séquences etc. Dans ce
manuscript, nous nous interesserons aux CRDTs pour séquences, la structure de
données la plus proche d'un document.

Un CRDT pour séquences est un type de données abstrait représentant une
séquence. Il propose deux opérations basiques: l'insertion et la suppression
d'un élement (e.g. un caractère, une ligne, un paragraphe etc.). Ces deux
opérations sont commutatives, c'est à dire qu'elles peuvent être intégrées dans
n'importe quel ordre, à une exception près : une opération de suppression d'un
élement spécifique doit toujours suivre l'insertion de ce dernier.

Les CRDTs pour séquences associent à chaque élement un identifiant unique et
immuable. Ainsi la séquence peut être représenté par un ensemble de couples
$\langle element,\, identifier \rangle$. Un ordre total sur l'ensemble des
identifiants permet de retrouver la séquence, et donc le document.  Lorsque
l'un des auteurs du document partagé éffectue une opération d'insertion, un
identifiant doit être généré en fonction des identifiants voisins à
l'insertion.

\subsection{Pierres tombales}
Une famille de CRDTs, appelés CRDTs à pierre tombale (ou certificat de décès)
(REFs ) réference explicitement les identifiants voisins à l'insertion dans
l'identifiant généré. Historiquement, le premier de ces CRDTs, et plus
représentatif, se nomme WOOT (REF).

Le principal problème de cette approche concerne les operations de suppression.
En effet, l'ordre des élements est garanti par leurs identifiants. En
particulier, cet ordonancement se base dans un premier temps sur les références
contenues dans les identifiants. Ainsi, on ne peut placer un élement et son
identifiant dans la sequence si l'une de ses références vient à manquer. C'est
pour cette raison qu'une suppression cache seulement cet identifiant à
l'utilisateur, alors qu'il se trouve toujours dans le modèle. En d'autres
termes, toutes les operations impactent le modèle de manière définitive. On
peut alors se retrouver avec une séquence apparement vide, alors qu'elle
contient un très grand nombre d'élements cachés.

Le second problème concerne la complexité temporelle de l'algorithme permettant
d'ordonner la séquence. Tout d'abord, elle dépend de la totalité des operations
effectuées sur la sequence. Ensuite, l'algorithme récursif, bien qu'élégant,
possède une complexité cubique.

\begin{algorithm}
  \input{./input/lseq/wootalgo.tex}
  \caption{\label{algo:woot}WOOT.}
\end{algorithm}

\begin{algorithm}
  \input{./input/lseq/wootrecursalgo.tex}
  \caption{\label{algo:wootrecurs}Cherche l'indice où insérer le nouvel
    élement dans WOOT.}
\end{algorithm}


\subsection{Identifiants à taille variable}
When a collaborator performs an insert operation, it first allocates the
identifier of the element to insert. For instance, let us consider a sequence
$QWTY$ with the unique, immutable, and totally ordered integer identifiers $1$,
$2$, $4$, $8$ respectively. A collaborator inserts the element $E$ between $W$
and $T$. The natural identifier that comes to mind is $3$. The resulting
sequence is $QWETY$. However, $R$ cannot be inserted between $E$ and $T$ since
$3$ and $4$ are contiguous. The space of identifiers must be enlarged to handle
the new insertion. If we consider identifiers as decimal numbers, $3.1$ can be
associated with the character $R$. If a new character has to be inserted
between $E$ and $R$, a new identifier will be allocated between $3$ and
$3.1$. Again, the space will be extended resulting in a new identifier $3.0$
suffixed by any non null integer. Let $X$ be the suffix, the order is preserved
since $(3 < 3.0.X < 3.1)$. Such growing identifiers are called variable-size
identifiers. The main objective is to keep the growth of the size of the
identifiers under acceptable boundaries.

Variable-size identifiers can be represented as a concatenation of basic
elements (e.g. integers). The resulting sequence can be represented by a tree
structure where the elements of the sequence are stored at the nodes and where
the edges of the tree are labelled such that a path from the root to a node
represents the identifier of the element stored at this node. For instance, the
character $R$ in the previous example is accessible following the path composed
of the edges labelled $3$ then $1$. More formally, a sequence is a tree
$\mathcal{T}$ where each node contains a value i.e. an element of the sequence
(over an alphabet $\mathcal{A}$). The tree $\mathcal{T}$ is a set of pairs
$\langle\mathcal{P}\subset\{\mathbb{N}\}^*,\, \mathcal{A} \rangle$, i.e., each
element has a path. Additionally, a total order
($\mathcal{P}$,$<_{\mathcal{P}}$) provides an ordering among the paths which
allows to retrieve the order of the elements in the sequence.

\noindent \textbf{Notation} A path composed of $p$ edges labelled
$\ell_1,\ldots,\ell_p$ will be noted $[\ell_1.\ldots.\ell_p]$.

Figure~\ref{fig:treemodelexample} shows the underlying 10-ary tree
$\mathcal{T}$ representing a sequences. Like in the previous scenario, the
initial sequence is $QWTY$ with the respective paths $[1]$, $[2]$, $[4]$ and
$[8]$. The insertion of the character $E$ between the pairs
$\langle [2],\, W\rangle$ and $\langle [4],\, T\rangle$ results in the
following pair: $\langle [3],\, E \rangle$. Then the insertion of the character
$R$ needs to start a new level since there is no room at the first level of the
tree for a new path between $E$ and $T$. The resulting path may be $[3.1]$ if
label one is chosen for the element $R$ at the second level. This would
increase the depth of the tree in case there is an insertion between the
elements $E$ and $R$. The new path would be $[3.0.X]$ where $0<X<10$ (recall
that we assumed a 10-ary tree). The total order $(\mathcal{P},\,<_\mathcal{P})$
allows retrieving the sequence $QWERTY$.

Two collaborators concurrently performing an operation on their respective
replica may get different results after the integration of both
operations. Indeed, $(\mathcal{P},\,<_\mathcal{P})$ is a total order when a
single collaborator edits. However, it becomes a partial order when the editing
involves several collaborators. Consequently, it is necessary to totally order
the elements inserted by different collaborators. To this end, the
disambiguation function $\delta$ associates a disambiguator to each pair of
$\mathcal{T}$. $\delta: \mathcal{P}\times\mathcal{A} \rightarrow \mathcal{D}$
such that ($\mathcal{D}$, $<_{\mathcal{D}}$) is a total order. The function
$\delta$ is an accessor to additional values that are totally ordered even in
presence of concurrency. Finally, the pairs in $\mathcal{T}$ can be totally
ordered with a composition of the local total order ($\mathcal{P}$,
$<_{\mathcal{P}}$) and the total order between collaborators ($\mathcal{D}$,
$<_{\mathcal{D}}$). The composition leads to a total order ($\mathcal{T}$,
$<_{\mathcal{T}}$).

Figure~\ref{fig:desexample} depicts a tree containing 6 elements with only 5
distinct paths (two values are associated with the path $[3]$). Similarly to
the previous example, the initial sequence was $QWTY$, however, in this
example, the two elements $E$ and $R$ are inserted between the pairs
$\langle [2],\, W\rangle$ and $\langle [4],\,T\rangle$ by two different
collaborators concurrently. For both elements, the resulting path is
$[3]$. After the two elements are inserted, the sequence becomes either
$QWERTY$ or $QWRETY$. Nevertheless, let
$\delta(\langle [3],\, R \rangle) = \delta_R$ and
$\delta(\langle [3],\, T\rangle) = \delta_T$. If we assume
$\delta_R <_\mathcal{D} \delta_T$, the total order
$(\mathcal{T}, <_\mathcal{T})$ gives the sequence $QWERTY$. It is worth noting
that disambiguators are usually computed using a monotonically increasing
variable and a unique collaborator identifier just like Lamport
timestamps~\cite{lamport1978time}. Therefore, a collaborator cannot directly
influence the final position of the character in the sequence using
disambiguators. The sequence of the example could have ended in $QWRETY$ and it
would have needed a correction.

The most critical part of sequences with variable-size identifiers consists in
creating the paths. Algorithm~\ref{algo:crdtabstract} shows the general
outlines of these sequences. It divides the operations (insert and delete) into
to different parts, the local and the remote phases of the optimistic
replication paradigm. As we can see, most of the core of the algorithm and the
associated complexity represents the local part of the $insert$ operation where
the algorithm has to generate a path (cf. Line~\ref{line:allocpath}) and a
disambiguator (cf. Line~\ref{line:allocdes}).

Let $\mathcal{I}$ be the set of unique triples
$\mathcal{I}: \mathcal{P}\times\mathcal{A}\times\mathcal{D}$. The set of the
elements of a sequence is a subset of $\mathcal{I}$. For any element $i$ of a
sequence ($i \in \mathcal{I}$), let $i.P$, $i.A$, $i.D$ be the respective
accessors to the path, the element, and the disambiguator of element $i$. The
function $allocPath$ chooses a path in the tree between two other paths $p.P$
and $q.P$ where $p<_{\mathcal{T}}q$. This means that element $i$ has to be
inserted between two elements $p$ and $q$ such that $p$ precedes $q$ in the
sequence. However, since it is always create new levels in the tree and thus
new sub-trees, the number of possible paths is infinite, and so is the number
of $allocPath$ functions. Nevertheless, the function $allocPath$ should choose
among the all the possible paths the one having the smallest length in order to
have smaller identifiers keeping good performance. This observation reduces
considerably the number of possible $allocPath$ functions. Still, the
allocation of paths without an \emph{a priori} knowledge of the final sequence
is a non-trivial problem (\ref{sec:problem} depicts the problem abstracted from
the collaborative editing context).

Figure~\ref{fig:allocpathexample} illustrates the difficulties of designing a
function to allocate the paths. It represents the underlying trees of two
sequences using the allocation function: they allocate the leftmost branch
available at the lowest depth possible. In both cases the final sequence is
$QWERTY$, however, the letters are not inserted in the same order. In the first
case, $Q$ is inserted first at position 0 (initially the sequence is empty),
followed by $W$ at position 1 (after $Q$) then $E$ is inserted at position 3
(after $W$), etc. We call the sequence of insert operations $[(Q,\,0)$,
$(W,\,1)$, $(E,\,2)$, $\ldots]$ the \emph{editing sequence}. In the second
case, the letter $Y$ is inserted first at position 0 as the sequence is
initially empty. Then $T$ is inserted. However as the final intended word is
$QWERTY$, $T$ has to be inserted at a position before $Y$ that represents the
current state of the sequence. $T$ is thus inserted at position 0 shifting $Y$
to position $1$, etc. The editing sequence that corresponds to this case
is thus $[(Y,\,0)$, $(T,\,0)$, $(R,\,0)$, $\ldots]$.

\begin{itemize}[leftmargin=*]
\item Case 1: Since the function $allocPath$ allocates the leftmost branches,
  the following editing sequence $[(Q,\,0)$, $(W,\,1)$, $(E,\,2)$, $\ldots]$
  leads to the paths $\langle [1],\, Q\rangle$, $\langle [2],\, W\rangle$,
  $\langle [3],\, E\rangle$, etc. In this case, the depth of the tree never
  grows. In this regard, this function $allocPath$ is very efficient in terms
  of the size of the allocated identifiers.

\item Case 2: The editing sequence $[(Y,\,0)$, $(T,\,0)$, $(R,\,0)$, $\ldots]$
  leads to an increase of the depth of the tree at each element
  insertion. Indeed, as an element gets the smallest value at its level
  (cf. the allocation function), there is no way to insert a new element before
  at the same level hence the new level. The resulting sequence is
  $\langle [1],\, Y\rangle$, $\langle [0.1],\, T\rangle$,
  $\langle [0.0.1],\, R \rangle$, etc. Consequently, the size of the paths
  grows very fast.
\end{itemize}

This example shows how the insertion order impacts the length of the allocated
paths. Unfortunately, the insertion order cannot be predicted, nor the size of
the final sequence. Prior work on sequences often made the assumption of a
left-to-right editing due to observations made on
corpus~\cite{preguica2009commutative, weiss2009logoot}. However, there exist
human edited documents that do not correspond to this kind of
editing~\cite{nedelec2013lseq}. Indeed, the editing depends on the type of the
document and to the activity for example when correcting a document the editing
in mainly random as the insertions/deletions may correspond to
errors. Consequently, we are looking for an allocation function which provides
identifiers with a sub-linear spatial complexity compared to the number of
insertions whatever is the editing sequence.

Distributed collaborative editing tools can be divided in two classes. The
Operational Transformation (OT) approach~\cite{saito2005optimistic} is the most
ancient. A wide variety of OT approaches exist for text editing, image editing
etc. In the context of text editing, OT can provide the usual $insert$ and
$delete$ operations plus a range of string-wise operations such as $move$,
$cut-paste$, etc. However, the correctness analysis requires to carefully
examine each pair of operations and their parameters. As a consequence, few OT
approaches are actually correct~\cite{imine2003proving}. Furthermore, this
first class can be divided into two subclasses centralised and
decentralised. Centralised approaches~\cite{nichols1995high} serialise the
order of operations on a central server easing the convergence. However, the
topology in itself implies a single point of failure, privacy issues, economic
intelligence issues, and scalability issues. Decentralised
approaches~\cite{sun2009contextbased} require a version vector to identify the
generation context of the received operations. It transforms the arguments of
the received operation against its concurrent and older operations to
consistently execute it without undoing the latter operations. While the local
operation is very efficient, the high price in case of concurrency must been
paid at all distant sites.  As a consequence, OT approaches work fine in
confined configurations only. However, they are not designed to handle massive
authoring of large documents. In comparison, EDITORNAME uses NAME that
does not require a logfile and whose complexity depends of the insert
operations only, sends only two scalars in messages to guarantee the
convergence, and supports concurrency.

The second class, besides the OT class, is the class based on conflict-free
replicated data types
(CRDTs)~\cite{shapiro2011comprehensive,shapiro2011conflict}. This class shares
the computational cost between the local and remote part of the optimistic
replication. While CRDTs significantly improve the time complexity compared to
decentralised OT approaches, they hide the complexity in the memory
usage. CRDTs for sequences can be slit into two sub-classes, the tombstone
class and the variable-size identifiers class. The tombstone 
CRDTs~\cite{ahmed2011evaluating,
  conway2014language,grishchenko2010deep,oster2006data,preguica2009commutative,
  roh2011replicated, weiss2007wooki,wu2010partial,Yu2012stringwise} marks the
deleted elements. Therefore, while these elements do not appear to the user,
they still exist in the underlying model and affect the overall
performance. Thus, the complexity depends on the number of insert and delete
operations. Including the deletes in the complexity is problematic because, for
instance, in Wikipedia documents subject to vandalism, delete operations are
very common. As a consequence, a page may contain few lines and yet use a large
amount of storage due to the hidden tombstones. On the other side, the
variable-size identifiers class of CRDTs~\cite{preguica2009commutative,
  andre2013supporting,weiss2009logoot} assigns an identifier to each element in
the document. Contrarily to tombstone approaches, deleted elements are truly
removed. However, the identifiers also encode the order of elements and hence
the space complexity of these approaches is crucial and depends only on the
number of insert operations.

In the literature, two kinds of allocation functions exist for the
identifiers. Let us consider an insertion of $b$ between two elements $a$ and
$c$; let $id_a$ and $id_c$ be their respective identifiers where
$id_a<id_c$. Both kinds of allocation functions aim to provide the shortest
identifiers possible. Thus, the maximum size of the identifier $id_b$
corresponding to $b$ is always: $min(|id_a|,\,|id_c|)+1$. The first allocation
function consists in randomising a path between $id_a$ and $id_c$. This
function aims to make the conflicts very unlikely. However, with a monotonic
editing behaviour, such function consumes on the average half the level of
identifiers in a branch. The second allocation function comes from the
observations made on a Wikipedia corpus that favours end-editing. When the
editing behaviour complies with this assumption, the average size of
identifiers remains linear. None of the two functions provides reasonable size
identifiers when associated with the dual editing behaviour.

The allocation function NAME~\cite{nedelec2013lseq,nedelec2013concurrency}
improves the state-of-the-art by lowering the space complexity of identifiers
from linear to sub-linear. Figure~\ref{fig:complexities} shows how it impacts
the average size of identifiers. As a consequence, NAME scales with respect
to the size of the document.

LogootSplit~\cite{andre2013supporting} proposes an orthogonal solution to
reduce the metadata generated by sequences with variable-size
identifiers. LogootSplit does not reduce the space complexity of the
identifiers but modifies the way identifiers are linked to elements. Indeed,
LogootSplit handles multiple granularities. LogootSplit aims to allocate
identifiers to coarse grain elements whenever it is possible in order to reduce
the number of identifiers. Being orthogonal, LogootSplit could use NAME to
benefit from its advantages.

Using NAME, a sequence still requires some form of causality
tracking. However, causality tracking is still an issue in distributed network
subject to churn~\cite{baldoni2002fundamentals}. A full causality tracking
requires piggybacking a version vector with $N$ entries within each message,
$N$ being the number of peers \emph{ever} involved in the network. Of course,
it is feasible in contexts where peers can join and leave the network
freely. Some other approaches~\cite{almeida2008interval} reuse entries of left
peers but require that leaving peers notify this clearly. Even though version
vectors (or other structures with the same space complexity) are necessary to
accurately track causality~\cite{charronbost1991concerning}, there exist
trade-offs between space complexity and accuracy. EDITORNAME chooses to
track semantically related operations accurately. The interval version
vector~\cite{mukund2014optimized} has a local space complexity eventually
comparable to the space complexity of the version vector while not overwhelming
the network with causality metadata.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../paper"
%%% End: 
